# Yes I'm Feelings
*Generative music piece built with Machine Learning from Radiohead's data by Sofía Suazo and Guillermo Montecinos for Generative Music class, NYU ITP Fall 2018.*

<p align="center">
  <a href="https://soundcloud.com/guillemontecinos/yeah-i-feelings">
    <img src="https://github.com/guillemontecinos/itp_fall_2018_generative_music/blob/master/week_6/documentation/yeah_I_feelings.jpg" align="middle" width="50%">
  </a>
</p>

Yes I'm feelings is 2:19 minutes a musical piece inspired in Radiohead's [Fitter Happier](https://www.youtube.com/watch?v=My10FLH5DT0) song from the album *Ok Computer*. It was build by predicting Radiohead-styled lyrics and melodies from two databases, which were live triggered from two computers. This piece was collaboration with [Sofía Suazo](https://www.sofialuisa.xyz/) for the midterm of the Generative Music class at NYU ITP, Fall 2018.

The piece's text were generated by a Markov Chain trained with Radiohead's lyrics and triggered in the browser using the library [p5.speech](http://ability.nyu.edu/p5.js-speech/), as is explained in the following [post](https://github.com/sofialuisa/GENERATIVE-MUSIC/tree/master/second_performance).

Besides, the music was generated using a [long short-term memory (LSMT)](https://en.wikipedia.org/wiki/Long_short-term_memory) recurrent neural network (RRN) trained with data collected from MIDI files. The original idea was to compose a Thom Yorke-styled melody from of a bunch of Radiohead's songs but the difficulty of properly interpreting time from MIDI files brought us to reinterpret the result of the prediction process.

## Data collection
For addressing the goal of predicting a Radiohead-style melody a [dataset](https://github.com/guillemontecinos/itp_fall_2018_generative_music/tree/master/week_6/dataset/radiohead/midi) of 18 MIDI files were collected from internet. Due to the MIDI protocol is old there is a huge variety and quality of files for the same song which brought us to manually clean the data set by listening every file using [MuseScore](https://musescore.com/) and checking the quality -or presence- of a melody in a separated track. After that process we came to a sub dataset composed by the following songs -which can be found in the dataset folder:

* 4_faust_arp
* 10_optimistic
* 11_everything_is_in_its_right_place
* 18_fake_plastic_trees
* 19_idioteque
* 21_knives_out
* codex
* exit_music_for_a_film
* just
* no_surprises
* paranoid_android
* videotape

## Data processing
For training the LSTM model MIDI data was converted into text sequences, process in which only two parameters were collected: pitch and time. The python library mido to import each MIDI message from a file and get from them the pitch and time of each note using this [script](https://github.com/guillemontecinos/itp_fall_2018_generative_music/blob/master/week_6/process_midi.py).

Each song was interpreted into a concatenation of words that were composed by the MIDI pitch in values from 0 - 127, and the time in ticks -each note looked like "66_238". The processed data can be found [here](https://github.com/guillemontecinos/itp_fall_2018_generative_music/blob/master/week_6/assets/). Finally, as the model training script used requires a unified dataset as an input, all songs were concatenated into one file called *input.txt*.

## Model training and simulations with ml5.js
For the model training process the [LSTM training framework](https://github.com/handav/lstm_training_and_generation) developed by Hannah Davis was used. This model was trained with the data gotten from the main melody of 12 Radiohead songs.

The melody was generated by running a JavaScript file in which through ml5.js' commands the model was used to generate new text sequences. Ten sequences were generated and then curated based on how similar they sound to Radiohead's melodies. Those sequences were used to trigger strings and drums in Ableton Live.
